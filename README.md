<img width="896" height="208" alt="image" src="https://github.com/user-attachments/assets/8970c4f2-33d3-4192-9a33-d0c21f1ace12" />




La nostra attivitÃ  progettuale per il corso di Fondamenti di Intelligenza Artificiale si Ã¨ basato sulla realizzazione di un sistema per la progettazione, lâ€™allenamento e la valutazione di unâ€™Intelligenza Artificiale capace di sfidare gli utenti al gioco di carte **Tresette**, tutto ciÃ² applicando tecniche di **Reinforcement Learning (RL)** e simulazioni di partite.

---

## ğŸ¯ Obiettivi

- **Modellare** le regole e le dinamiche del gioco del Tresette. -> Essere in grado di giocare contro un'AI non allenata(random)
- **Definire** un ambiente simulato in cui un agente AI possa giocare e migliorare.  -> Effettuare un training
- **Addestrare** lâ€™agente tramite algoritmi di *Deep Reinforcement Learning* (es. Deep Q-Learning), attraverso simulazioni di partite ed esplorazione contro AI che inizialmente gioca in random, poi utilizza un heuristica e in fine con una fase DQN pura
- **Permettere** a un giocatore umano di sfidare lâ€™AI.  
- **Confrontare** strategie casuali, euristiche e di apprendimento.  

---

## ğŸƒ Regole implementate

- Gestione del **mazzo** e distribuzione delle carte.  
- Turni di gioco per **4 giocatori** attraverso lista di mosse legali.  
- Calcolo delle **prese** e dei **punteggi** secondo le regole del Tresette. -> 1/3 per le figure , 1 per l'asso
- Definizione delle **condizioni di vittoria**.  

---

## ğŸ§  Architettura AI

- **Ambiente** â†’ rappresenta lo stato completo della partita (mani, prese, turno, carte giocate e segnali).  
- **Agente** â†’ apprende a selezionare la mossa ottimale attraverso una `Îµ-greedy policy`, bilanciando esplorazione e sfruttamento.  
- **Rete DQN** â†’ stima i valori Q delle azioni possibili e aggiorna i pesi della rete tramite backpropagation.  
- **Reward shaping** â†’ fornisce ricompense dense per incoraggiare prese vantaggiose e penalizzare mosse deboli o errori strategici.  
- **Training loop** â†’ simula migliaia di partite, aggiornando la policy e salvando checkpoint periodici per monitorare lâ€™evoluzione dellâ€™agente.   

---
## ğŸ‹ï¸â€â™‚ï¸ Training

Il processo di training dellâ€™agente segue 3 fasi principali:  

Il processo di training dellâ€™agente segue 3 fasi principali:  

1. **Fase casuale (warm-up iniziale)** â†’ lâ€™agente gioca utilizzando mosse casuali o parzialmente guidate da regole semplici, cosÃ¬ da esplorare lo spazio delle possibilitÃ  e riempire il replay buffer con le prime esperienze.  

2. **Fase euristica (pre-training)** â†’ in questa fase, lâ€™agente segue esclusivamente la logica dellâ€™euristica, che privilegia mosse piÃ¹ sensate (es. conservare gli assi, evitare di sprecare carte forti). Il DQN osserva queste partite e impara da esempi coerenti.  

3. **Fase DQN pura (sfruttamento)** â†’ una volta terminato il pre-training, lâ€™agente utilizza solo la rete neurale per scegliere le mosse, basandosi sui valori Q stimati. In questa fase non avviene piÃ¹ esplorazione casuale: lâ€™AI gioca in modo deterministico, sfruttando al massimo la policy appresa.  

Durante il training:  
- Possono essere salvati **checkpoint periodici** del modello, per poter riprendere lâ€™allenamento senza perdere i progressi.  
- Se disponibile, viene utilizzata la **GPU** tramite **PyTorch** per velocizzare il processo di apprendimento.  

 I parametri di training (es. numero di episodi, learning rate, epsilon decay, frequenza dei checkpoint) possono essere modificati direttamente nel file train_dqn.py.
 
 Abbiamo sfruttato le **infrastrutture HPC fornite da UniversitÃ  di Bologna (CS UNIBO)** per il training dellâ€™agente, in particolare utilizzando una GPU NVIDIA L40 presente nella partizione â€œl40â€. Questa configurazione ha permesso di accelerare significativamente lâ€™addestramento del modello DQN garantendo tempi di calcolo adeguati e sfruttando al meglio il batch-processing parallelo.

Per avviare il training:  
```bash
python train_dqn.py
```
---

## ğŸ§© Architettura generale

Il sistema Ã¨ strutturato come un classico ambiente di **Reinforcement Learning**:  

| Componente | Descrizione |
|-------------|-------------|
| ğŸ§  Agente | Decide le mosse in base ai valori Q stimati dalla rete neurale. |
| ğŸ® Ambiente | Simula lo stato del gioco, gestendo mani, prese, punteggi e regole del Tresette. |
| ğŸª™ Reward shaping | Introduce ricompense intermedie per incoraggiare comportamenti utili (es. vincere prese, evitare errori). |
| ğŸ§© Rete neurale (DQN) | Predice i Q-values per ogni azione possibile e aggiorna i pesi durante il training. |
| ğŸ” Replay Buffer | Memorizza le esperienze (stato, azione, ricompensa, stato successivo) per stabilizzare lâ€™apprendimento. |
| ğŸ§® Target Network | Copia periodicamente i pesi della rete principale per evitare oscillazioni e migliorare la convergenza. |
| âš™ï¸ Epsilon Decay | Riduce gradualmente la casualitÃ  nelle scelte, passando da esplorazione a sfruttamento. |
| ğŸ§‘â€ğŸ« Agente Euristico | Fornisce esempi iniziali sensati per accelerare lâ€™apprendimento dellâ€™agente DQN. |


---

## ğŸ“‚ Struttura del progetto

```bash
Tresette_AI/
â”œâ”€ cards.py                  # Definizione e utilitÃ  per le carte
â”œâ”€ rules.py                  # Regole del Tresette e calcolo dei punteggi
â”œâ”€ game4p.py                 # Logica del gioco a 4 giocatori e gestione dei turni
â”œâ”€ obs/encoder.py            # Codifica dello stato di gioco in feature numeriche
â”œâ”€ utils/HeuristicAgent.py   # Agente basato su regole euristiche
â”œâ”€ train_dqn.py              # Training DQN, gestione checkpoint e salvataggi
â”œâ”€ watch_game.py             # Simulazione e visualizzazione di partite singole
â”œâ”€ watch_game_parallel.py    # Esecuzione di tornei paralleli (batch GPU)
â”œâ”€ menu_cli.py               # Interfaccia a riga di comando per giocare contro lâ€™AI
â”œâ”€ train_long.sbatch         # Script per esecuzione su cluster HPC (GPU L40 nel nostro caso)
â”œâ”€ requirements.txt          # Dipendenze del progetto
â””â”€ README.md                 # Documentazione principale
```
---

## ğŸš€ Installazione

Per installare ed eseguire il progetto:

```bash
# 1) Clona la repo
git clone https://github.com/BlackRaffo70/Tresette_AI.git
cd Tresette_AI

# 2) Crea e attiva un ambiente virtuale
python -m venv venv
source venv/bin/activate        # Mac/Linux
# .\venv\Scripts\activate       # Windows

# 3) Installa le dipendenze
pip install -r requirements.txt

# 4) (opzionale) Verifica GPU in PyTorch
python - << 'EOF'
import torch
print("CUDA disponibile:", torch.cuda.is_available())
EOF
```
---
## ğŸ® Giocare contro lâ€™AI

Dopo aver completato il training, Ã¨ possibile sfidare lâ€™agente tramite lâ€™interfaccia a riga di comando:

```bash
python menu_cli.py
```

Oppure osservare partite simulate:

```bash
python Watch_game.py
```
Valutazione piÃ¹ rapida (batch, ideale su GPU)

```bash
python Watch_game_parallel.py
```


## ğŸ“¦ Requisiti

Il progetto richiede **Python 3.10+** e le seguenti librerie principali:

```txt
Python >= 3.10
torch >= 2.0.0
numpy >= 1.23.0
matplotlib >= 3.7.0
tqdm >= 4.65.0
gym >= 0.26.0
```
---

# ğŸ‘¥ Autori

| | |
|:--:|:--:|
| <a href="https://github.com/BlackRaffo70"><img src="https://github.com/BlackRaffo70.png" width="110" alt="avatar Raffaele Neri"></a> | <a href="https://github.com/sebastianogiannitti"><img src="https://github.com/sebastianogiannitti.png" width="110" alt="avatar Sebastiano Giannitti"></a> |
| **Raffaele Neri**<br/>[@BlackRaffo70](https://github.com/BlackRaffo70) | **Sebastiano Giannitti**<br/>[@sebastianogiannitti](https://github.com/sebastianogiannitti) |

