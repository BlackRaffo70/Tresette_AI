# ğŸƒ Tresette AI

La nostra attivitÃ  progettuale per il corso di Fondamenti di Intelligenza Artificiale si Ã¨ basato sulla realizzazione di un sistema per la progettazione, lâ€™allenamento e la valutazione di unâ€™Intelligenza Artificiale capace di sfidare gli utenti al gioco di carte **Tresette**, tutto ciÃ² applicando tecniche di **Reinforcement Learning (RL)** e simulazioni di partite.

---

## ğŸ¯ Obiettivi

- **Modellare** le regole e le dinamiche del gioco del Tresette. -> Essere in grado di giocare contro un'AI non allenata(random)
- **Definire** un ambiente simulato in cui un agente AI possa giocare e migliorare.  -> Effettuare un training
- **Addestrare** lâ€™agente tramite algoritmi di *Deep Reinforcement Learning* (es. Deep Q-Learning).  
- **Permettere** a un giocatore umano di sfidare lâ€™AI.  
- **Confrontare** strategie casuali, euristiche e di apprendimento.  

---

## ğŸƒ Regole implementate

- Gestione del **mazzo** e distribuzione delle carte.  
- Turni di gioco per **4 giocatori** attraverso lista di mosse legali.  
- Calcolo delle **prese** e dei **punteggi** secondo le regole del Tresette. -> 1/3 per le figure , 1 per l'asso
- Definizione delle **condizioni di vittoria**.  

---

## ğŸ§  Architettura AI

- **Ambiente** â†’ rappresenta lo stato della partita (mani, prese, turno, carte giocate).  
- **Agente** â†’ apprende a selezionare le mosse tramite `Îµ-greedy policy`.  
- **Reward shaping** â†’ ricompense intermedie per prese utili e penalitÃ  per errori.  
- **Training loop** â†’ simulazione di migliaia di partite per ottimizzare la policy.  

---
## ğŸ‹ï¸â€â™‚ï¸ Training

Il processo di training dellâ€™agente segue due fasi principali:  

1. **Fase casuale** â†’ lâ€™agente gioca utilizzando mosse casuali, in modo da esplorare lo spazio delle possibilitÃ  e raccogliere esperienza.  
2. **Fase euristica** â†’ successivamente, viene introdotta una semplice euristica che guida le scelte dellâ€™agente (es. preferire mosse con carte forti o evitare sprechi), accelerando lâ€™apprendimento prima che intervenga lâ€™ottimizzazione tramite **Deep Q-Learning**.  

Durante il training:  
- Vengono salvati **checkpoint periodici** del modello, per poter riprendere lâ€™allenamento senza perdere i progressi.  
- Se disponibile, viene utilizzata la **GPU** tramite **PyTorch** per velocizzare il processo di apprendimento.  

 I parametri di training (es. numero di episodi, learning rate, epsilon decay, frequenza dei checkpoint) possono essere modificati direttamente nel file train_dqn.py.

Per avviare il training:  
```bash
python train_dqn.py
```
---

## ğŸ“‚ Struttura del progetto

```bash
Tresette_AI/
â”‚â”€â”€ .venv/              # Ambiente virtuale (non incluso nel repo)
â”‚
â”‚â”€â”€ obs/                # Moduli per osservazioni e rappresentazioni dello stato
â”‚   â””â”€â”€ encoder.py      # Encoder per trasformare lo stato in input per l'AI
â”‚
â”‚â”€â”€ tests/              # Test automatici e unit test
â”‚
â”‚â”€â”€ cards.py            # Rappresentazione e utilitÃ  per le carte
â”‚â”€â”€ game4p.py           # Gestione del gioco a 4 giocatori
â”‚â”€â”€ menu_cli.py         # Interfaccia a riga di comando per giocare
â”‚â”€â”€ rules.py            # Regole del Tresette e funzioni di punteggio
â”‚â”€â”€ train_dqn.py        # Script di training con Deep Q-Learning
â”‚
â””â”€â”€ README.md           # Documentazione del progetto
```
---
## ğŸš€ Installazione

Per installare ed eseguire il progetto:

```bash
# 1. Clona la repository
git clone https://github.com/tuo-username/Tresette_AI.git
cd Tresette_AI

# 2. Crea e attiva un ambiente virtuale
python -m venv .venv
source .venv/bin/activate   # Mac/Linux
.venv\Scripts\activate      # Windows

# 3. Installa le dipendenze
pip install -r requirements.txt

# 4. (Opzionale) Verifica il supporto GPU con PyTorch
python - <<EOF
import torch
print("CUDA disponibile:", torch.cuda.is_available())
EOF
```
---
## ğŸ® Giocare contro lâ€™AI

Dopo aver completato il training, Ã¨ possibile sfidare lâ€™agente tramite lâ€™interfaccia a riga di comando:

```bash
python menu_cli.py
```

